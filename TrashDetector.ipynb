{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout,Conv2D,BatchNormalization,LeakyReLU,Concatenate,UpSampling2D,MaxPool2D,Add\n",
    "from keras import backend as K\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 26\n",
    "NUM_ANCHORS = 3\n",
    "constant_anchors = {\n",
    "    #\"52\":[[10,13], [16,30], [33,23]],\n",
    "    26:[[7.90196078,5.824],[ 17.46078431,16.19298246],[ 43.03448105,39.25490196]],\n",
    "    13:[[ 95.50221204,73.70000839],[145.99999905,161.99999809],[255.31603053,238.99232006]],\n",
    "}\n",
    "\n",
    "ANCHORS = np.array([[7.90196078,5.824],[ 17.46078431,16.19298246],[ 43.03448105,39.25490196],[ 95.50221204,73.70000839],[145.99999905,161.99999809],[255.31603053,238.99232006]])\n",
    "\n",
    "ANCHOR_INDECES = {\n",
    "    26:[0,1,2],\n",
    "    13:[3,4,5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30294be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Mish(layers.Layer):\n",
    "    \"\"\"Mish activation\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Mish, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * K.tanh(K.softplus(inputs))\n",
    "\n",
    "\n",
    "# this GroupFeature can not work because it cause some bugs:\n",
    "# 'number of input channels does not match corresponding dimension of filter, 32!=64'\n",
    "# class GroupFeature(layers.Layer):\n",
    "#\n",
    "#     def __init__(self, num_splits, **kwargs):\n",
    "#         super(GroupFeature, self).__init__(**kwargs)\n",
    "#         self.num_splits = num_splits\n",
    "#\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         return tf.split(inputs, num_or_size_splits=self.num_splits, axis=-1)[0]\n",
    "\n",
    "\n",
    "def conv_bn_activation(inputs,\n",
    "                       filters,\n",
    "                       filter_size,\n",
    "                       downsample=False,\n",
    "                       activation='leaky'):\n",
    "    \"\"\"yolo4-tiny is using leaky activation in source code\"\"\"\n",
    "\n",
    "    assert activation in ['mish', 'leaky'], 'activation must be leaky or mish'\n",
    "\n",
    "    if downsample:\n",
    "        inputs = layers.ZeroPadding2D(padding=((1, 0), (1, 0)))(inputs)\n",
    "        padding = 'valid'\n",
    "        strides = 2\n",
    "    else:\n",
    "        padding = 'same'\n",
    "        strides = 1\n",
    "\n",
    "    x = layers.Conv2D(filters=filters,\n",
    "                      kernel_size=filter_size,\n",
    "                      strides=strides,\n",
    "                      padding=padding,\n",
    "                      use_bias=False,\n",
    "                      kernel_regularizer=keras.regularizers.l2(0.0005),\n",
    "                      kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),\n",
    "                      bias_initializer=keras.initializers.constant(0.0))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation == 'mish':\n",
    "        x = Mish()(x)\n",
    "    else:\n",
    "        x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def csp_darknet_tiny(inputs):\n",
    "    x = conv_bn_activation(inputs, 32, 3, downsample=True)\n",
    "    x = conv_bn_activation(x, 64, 3, downsample=True)\n",
    "    x = conv_bn_activation(x, 64, 3)\n",
    "\n",
    "    route = x\n",
    "\n",
    "    # TODO: the index is 1 in source code so i change 0 to 1\n",
    "    x_group = layers.Lambda(lambda y: tf.split(y, num_or_size_splits=2, axis=-1)[1])(x)\n",
    "    x_group = conv_bn_activation(x_group, 32, 3)\n",
    "    route1 = x_group\n",
    "    x_group = conv_bn_activation(x_group, 32, 3)\n",
    "    x_group = layers.Concatenate()([x_group, route1])\n",
    "    x_group = conv_bn_activation(x_group, 64, 1)\n",
    "    x_group = layers.Concatenate()([route, x_group])\n",
    "    x_group = layers.MaxPool2D(pool_size=2, padding='same')(x_group)\n",
    "\n",
    "    x = conv_bn_activation(x_group, 128, 3)\n",
    "    route = x\n",
    "\n",
    "    # TODO: the index is 1 in source code so i change 0 to 1\n",
    "    x_group = layers.Lambda(lambda y: tf.split(y, num_or_size_splits=2, axis=-1)[1])(x)\n",
    "    x_group = conv_bn_activation(x_group, 64, 3)\n",
    "    route1 = x_group\n",
    "    x_group = conv_bn_activation(x_group, 64, 3)\n",
    "    x_group = layers.Concatenate()([x_group, route1])\n",
    "    x_group = conv_bn_activation(x_group, 128, 1)\n",
    "    x_group = layers.Concatenate()([route, x_group])\n",
    "    x_group = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(x_group)\n",
    "\n",
    "    x = conv_bn_activation(x_group, 256, 3)\n",
    "    route = x\n",
    "\n",
    "    # TODO: the index is 1 in source code so i change 0 to 1\n",
    "    x_group = layers.Lambda(lambda y: tf.split(y, num_or_size_splits=2, axis=-1)[1])(x)\n",
    "    x_group = conv_bn_activation(x_group, 128, 3)\n",
    "    route1 = x_group\n",
    "    x_group = conv_bn_activation(x_group, 128, 3)\n",
    "    x_group = layers.Concatenate()([x_group, route1])\n",
    "    x_group = conv_bn_activation(x_group, 256, 1)\n",
    "\n",
    "    C4 = x_group\n",
    "\n",
    "    x_group = layers.Concatenate()([route, x_group])\n",
    "    x_group = layers.MaxPool2D(pool_size=2, strides=2)(x_group)\n",
    "\n",
    "    C5 = conv_bn_activation(x_group, 512, 3)\n",
    "\n",
    "    return C4, C5\n",
    "\n",
    "\n",
    "def yolo4_tiny(inputs, num_anchors, num_classes):\n",
    "\n",
    "    C4, C5 = csp_darknet_tiny(inputs)\n",
    "\n",
    "    x = conv_bn_activation(C5, 256, 1)\n",
    "    output_C5 = conv_bn_activation(x, 512, 3)\n",
    "    output_C5 = layers.Conv2D(num_anchors * (num_classes + 5),\n",
    "                              1,\n",
    "                              kernel_regularizer=keras.regularizers.l2(5e-4),\n",
    "                              kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),\n",
    "                              bias_initializer=keras.initializers.constant(0.0))(output_C5)\n",
    "\n",
    "    x_upsample = conv_bn_activation(x, 128, 1)\n",
    "    x_upsample = layers.UpSampling2D()(x_upsample)\n",
    "    x_concat = layers.Concatenate()([x_upsample, C4])\n",
    "\n",
    "    output_C4 = conv_bn_activation(x_concat, 256, 3)\n",
    "    output_C4 = layers.Conv2D(num_anchors * (5 + num_classes),\n",
    "                              1,\n",
    "                              kernel_regularizer=keras.regularizers.l2(5e-4),\n",
    "                              kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),\n",
    "                              bias_initializer=keras.initializers.constant(0.0))(output_C4)\n",
    "\n",
    "    print(output_C4.shape, output_C5.shape)\n",
    "    model = keras.Model(inputs, [output_C5, output_C4])\n",
    "\n",
    "    return model\n",
    "\n",
    "inputs = keras.Input(shape=(416, 416, 3))\n",
    "backbone = yolo4_tiny(inputs, num_anchors=NUM_ANCHORS, num_classes=NUM_CLASSES)\n",
    "backbone.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd32e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"annotations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    official_data = json.load(f)\n",
    "with open(\"annotations_unofficial.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    unofficial_data = json.load(f)\n",
    "print([key for key,content in official_data.items()])\n",
    "def getData(selected_json):\n",
    "    list_of_links = 1500 * [0]\n",
    "    for dict in selected_json[\"images\"]:\n",
    "        list_of_links[dict['id']] = {'link':dict['flickr_url'],'height':dict['height'],'width':dict['width']}\n",
    "\n",
    "    official_bbox = 1500 * [0]\n",
    "    for dict in selected_json[\"annotations\"]:\n",
    "        #print(dict)\n",
    "        coco_format = [dict[\"bbox\"][0],dict[\"bbox\"][1],dict[\"bbox\"][2],dict[\"bbox\"][3]]\n",
    "        if coco_format[0] < 0:\n",
    "            coco_format[2] += coco_format[0]  # shrink width (x is negative)\n",
    "            coco_format[0] = 0   # clamp x\n",
    "        if coco_format[1] < 0:\n",
    "            coco_format[3] += coco_format[1]  # shrink height (y is negative)\n",
    "            coco_format[1] = 0   # clamp y\n",
    "        coco_format[2] = min(coco_format[2], list_of_links[dict[\"image_id\"]]['width'] - coco_format[0])\n",
    "        coco_format[3] = min(coco_format[3], list_of_links[dict[\"image_id\"]]['height'] - coco_format[1])\n",
    "\n",
    "        if official_bbox[dict[\"image_id\"]] == 0:\n",
    "            official_bbox[dict[\"image_id\"]] = {\"image_data\":list_of_links[dict[\"image_id\"]],\"bboxes\":[{'category':dict[\"category_id\"],'x':coco_format[0],'y':coco_format[1],'w':coco_format[2],'h':coco_format[3]}]}\n",
    "        else:\n",
    "            official_bbox[dict[\"image_id\"]][\"bboxes\"].append({'category':dict[\"category_id\"],'x':coco_format[0],'y':coco_format[1],'w':coco_format[2],'h':coco_format[3]})\n",
    "    return official_bbox\n",
    "\n",
    "official_data_formated = getData(official_data)\n",
    "unofficial_data_formated = getData(official_data)\n",
    "\n",
    "print(official_data_formated[0])\n",
    "print(official_data_formated[1])\n",
    "print(official_data_formated[2])\n",
    "print(official_data_formated[2][\"bboxes\"])\n",
    "print(official_data[\"categories\"])\n",
    "\n",
    "print(official_data[\"annotations\"][2][\"bbox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHOTO_SIZE = 416\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def iou_vectorized(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute IoU between every pair of boxes in boxes1 and boxes2.\n",
    "    boxes1: (N, 4) [x_center, y_center, w, h]\n",
    "    boxes2: (M, 4) [x_center, y_center, w, h]\n",
    "    Returns:\n",
    "      iou: (N, M) tensor with IoU values.\n",
    "    \"\"\"\n",
    "    \n",
    "    boxes1_corners = box_centers_to_corners(boxes1)  # (N,4)\n",
    "    boxes2_corners = box_centers_to_corners(boxes2)  # (M,4)\n",
    "\n",
    "    # Expand dims to broadcast pairwise\n",
    "    boxes1_exp = tf.expand_dims(boxes1_corners, 1)  # (N,1,4)\n",
    "    boxes2_exp = tf.expand_dims(boxes2_corners, 0)  # (1,M,4)\n",
    "\n",
    "    # Intersection coordinates\n",
    "    inter_xmin = tf.maximum(boxes1_exp[..., 0], boxes2_exp[..., 0])  # (N,M)\n",
    "    inter_ymin = tf.maximum(boxes1_exp[..., 1], boxes2_exp[..., 1])  # (N,M)\n",
    "    inter_xmax = tf.minimum(boxes1_exp[..., 2], boxes2_exp[..., 2])  # (N,M)\n",
    "    inter_ymax = tf.minimum(boxes1_exp[..., 3], boxes2_exp[..., 3])  # (N,M)\n",
    "\n",
    "    # Intersection dimensions\n",
    "    inter_w = tf.maximum(inter_xmax - inter_xmin, 0)\n",
    "    inter_h = tf.maximum(inter_ymax - inter_ymin, 0)\n",
    "    inter_area = inter_w * inter_h  # (N,M)\n",
    "\n",
    "    # Areas of boxes\n",
    "    area1 = (boxes1_corners[..., 2] - boxes1_corners[..., 0]) * (boxes1_corners[..., 3] - boxes1_corners[..., 1])  # (N,)\n",
    "    area2 = (boxes2_corners[..., 2] - boxes2_corners[..., 0]) * (boxes2_corners[..., 3] - boxes2_corners[..., 1])  # (M,)\n",
    "\n",
    "    area1 = tf.expand_dims(area1, 1)  # (N,1)\n",
    "    area2 = tf.expand_dims(area2, 0)  # (1,M)\n",
    "\n",
    "    union_area = area1 + area2 - inter_area  # (N,M)\n",
    "\n",
    "    iou = tf.where(union_area > 0, inter_area / union_area, tf.zeros_like(inter_area))\n",
    "\n",
    "    return iou\n",
    "\n",
    "def box_centers_to_corners(boxes):\n",
    "    \"\"\"\n",
    "    Convert [x_center, y_center, w, h] to [x_min, y_min, x_max, y_max].\n",
    "    boxes: (N, 4) tensor.\n",
    "    Returns: (N, 4) tensor.\n",
    "    \"\"\"\n",
    "    x_c, y_c, w, h = tf.split(boxes, 4, axis=-1)  # each (N,1)\n",
    "    x_min = x_c - w / 2\n",
    "    y_min = y_c - h / 2\n",
    "    x_max = x_c + w / 2\n",
    "    y_max = y_c + h / 2\n",
    "    return tf.concat([x_min, y_min, x_max, y_max], axis=-1)  # (N,4)\n",
    "\n",
    "def ciou(boxes1, boxes2, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate CIoU between boxes1 and boxes2\n",
    "\n",
    "    boxes1, boxes2: [..., 4], format = [x_center, y_center, width, height], normalized coords (0~1)\n",
    "    returns: tensor [...], CIoU values\n",
    "    \"\"\"\n",
    "    boxes1 = tf.convert_to_tensor(boxes1)\n",
    "    boxes2 = tf.convert_to_tensor(boxes2)\n",
    "\n",
    "    # Convert centers to corners: (x1, y1, x2, y2)\n",
    "    boxes1_x1 = boxes1[..., 0] - boxes1[..., 2] / 2\n",
    "    boxes1_y1 = boxes1[..., 1] - boxes1[..., 3] / 2\n",
    "    boxes1_x2 = boxes1[..., 0] + boxes1[..., 2] / 2\n",
    "    boxes1_y2 = boxes1[..., 1] + boxes1[..., 3] / 2\n",
    "\n",
    "    boxes2_x1 = boxes2[..., 0] - boxes2[..., 2] / 2\n",
    "    boxes2_y1 = boxes2[..., 1] - boxes2[..., 3] / 2\n",
    "    boxes2_x2 = boxes2[..., 0] + boxes2[..., 2] / 2\n",
    "    boxes2_y2 = boxes2[..., 1] + boxes2[..., 3] / 2\n",
    "\n",
    "    # Intersection box\n",
    "    inter_x1 = tf.maximum(boxes1_x1, boxes2_x1)\n",
    "    inter_y1 = tf.maximum(boxes1_y1, boxes2_y1)\n",
    "    inter_x2 = tf.minimum(boxes1_x2, boxes2_x2)\n",
    "    inter_y2 = tf.minimum(boxes1_y2, boxes2_y2)\n",
    "\n",
    "    inter_w = tf.maximum(inter_x2 - inter_x1, 0)\n",
    "    inter_h = tf.maximum(inter_y2 - inter_y1, 0)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # Areas\n",
    "    area1 = (boxes1_x2 - boxes1_x1) * (boxes1_y2 - boxes1_y1)\n",
    "    area2 = (boxes2_x2 - boxes2_x1) * (boxes2_y2 - boxes2_y1)\n",
    "\n",
    "    union_area = area1 + area2 - inter_area + eps\n",
    "    #inter_area = tf.cast(inter_area, tf.float32)\n",
    "    #union_area = tf.cast(union_area, tf.float32)\n",
    "\n",
    "    iou = tf.where(\n",
    "        tf.not_equal(union_area, 0.0),\n",
    "        inter_area / union_area,\n",
    "        tf.constant(0.0, dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    # center distance squared\n",
    "    center_dist = tf.square(boxes1[..., 0] - boxes2[..., 0]) + tf.square(boxes1[..., 1] - boxes2[..., 1])\n",
    "\n",
    "    # smallest enclosing box\n",
    "    enclose_x1 = tf.minimum(boxes1_x1, boxes2_x1)\n",
    "    enclose_y1 = tf.minimum(boxes1_y1, boxes2_y1)\n",
    "    enclose_x2 = tf.maximum(boxes1_x2, boxes2_x2)\n",
    "    enclose_y2 = tf.maximum(boxes1_y2, boxes2_y2)\n",
    "    enclose_w = enclose_x2 - enclose_x1\n",
    "    enclose_h = enclose_y2 - enclose_y1\n",
    "    c2 = tf.square(enclose_w) + tf.square(enclose_h) + eps\n",
    "\n",
    "    # aspect ratio consistency\n",
    "    w1 = boxes1[..., 2]\n",
    "    h1 = boxes1[..., 3]\n",
    "    w2 = boxes2[..., 2]\n",
    "    h2 = boxes2[..., 3]\n",
    "\n",
    "    v = (4 / (3.14159265 ** 2)) * tf.square(tf.math.atan(w2 / (h2 + eps)) - tf.math.atan(w1 / (h1 + eps)))\n",
    "    with tf.device('/CPU:0'):  # to avoid potential GPU precision errors\n",
    "        alpha = v / (1 - iou + v + eps)\n",
    "\n",
    "    ciou = iou - (center_dist / c2) - alpha * v\n",
    "\n",
    "    return ciou\n",
    "\n",
    "def focal_bce(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "    alpha_weight = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    return alpha_weight * modulating_factor * bce\n",
    "\n",
    "def vectorized_what_grid_cell_it_resides(true_BBs, resolution):\n",
    "    \"\"\"\n",
    "    true_BBs: tf.Tensor of shape (N, 4), each row = [x, y, w, h]\n",
    "    resolution: int, e.g., 13 or 26\n",
    "    \n",
    "    Returns:\n",
    "        x_cell: tf.Tensor, shape (N,), int32\n",
    "        y_cell: tf.Tensor, shape (N,), int32\n",
    "        normalized_BB: tf.Tensor, shape (N, 4), float32\n",
    "            Each row: [x_offset_in_cell, y_offset_in_cell, w, h]\n",
    "    \"\"\"\n",
    "    pixels_per_grid = PHOTO_SIZE // resolution  # scalar int\n",
    "\n",
    "    x = true_BBs[:, 0]  # shape (N,)\n",
    "    y = true_BBs[:, 1]  # shape (N,)\n",
    "    w = true_BBs[:, 2]  # shape (N,)\n",
    "    h = true_BBs[:, 3]  # shape (N,)\n",
    "\n",
    "    x_cell = tf.cast(x // pixels_per_grid, tf.int32)  # (N,)\n",
    "    y_cell = tf.cast(y // pixels_per_grid, tf.int32)  # (N,)\n",
    "\n",
    "    x_offset = x % pixels_per_grid  # (N,)\n",
    "    y_offset = y % pixels_per_grid  # (N,)\n",
    "\n",
    "    normalized_BB = tf.stack([x_offset, y_offset, w, h], axis=1)  # (N,4)\n",
    "\n",
    "    return x_cell, y_cell, normalized_BB\n",
    "\n",
    "def normalize_yolo_output(y_pred):\n",
    "    \"\"\"\n",
    "    y_pred: (batch, S, S, 3, 5 + C)\n",
    "    anchors: (3, 2) — anchor box sizes (width, height) in pixels\n",
    "    grid_size: int — S = 13 or 26 typically\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch, S, S, 3, 5 + C) with:\n",
    "        - objectness: sigmoid\n",
    "        - class scores: softmax\n",
    "    \"\"\"\n",
    "    grid_size = y_pred.shape[1]\n",
    "    anchors = tf.convert_to_tensor(constant_anchors[grid_size], dtype=tf.float32)  # (3, 2)\n",
    "\n",
    "    # Ensure anchor shape is (1, 1, 1, 3, 2) to broadcast\n",
    "    anchors = tf.reshape(anchors, (1, 1, 1, NUM_ANCHORS, 2))\n",
    "\n",
    "    # Slice components\n",
    "    tx = tf.sigmoid(y_pred[..., 0]) * (PHOTO_SIZE // grid_size)\n",
    "    ty = tf.sigmoid(y_pred[..., 1]) * (PHOTO_SIZE // grid_size)\n",
    "    tw = tf.exp(y_pred[..., 2]) * anchors[..., 0]\n",
    "    th = tf.exp(y_pred[..., 3]) * anchors[..., 1]\n",
    "    obj = tf.sigmoid(y_pred[..., 4])\n",
    "    cls = tf.nn.softmax(y_pred[..., 5:], axis=-1)\n",
    "\n",
    "    # Stack normalized values\n",
    "    bbox = tf.stack([tx, ty, tw, th, obj], axis=-1)  # shape: (batch, S, S, 3, 5)\n",
    "    out = tf.concat([bbox, cls], axis=-1)           # shape: (batch, S, S, 3, 5 + C)\n",
    "\n",
    "    return out\n",
    "\n",
    "@tf.function\n",
    "def yolo_loss_single_head(y_true, y_pred,\n",
    "                            lambda_coord = 20.0,\n",
    "                            lambda_obj = 10.0,\n",
    "                            lambda_noobj = 0.5,\n",
    "                            lambda_class = 3.0):\n",
    "    \"\"\"\n",
    "    y_true: (batch, MAX_BOUNDING_BOXES, 5 + C)\n",
    "    y_pred: (batch, S, S, B, 5 + C)\n",
    "    anchors: Tensor shape (B, 2) → width, height\n",
    "    \"\"\"\n",
    "\n",
    "    resolution = y_pred.shape[1]\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "\n",
    "    total_loss = 0\n",
    "    position_loss = 0\n",
    "    obj_confidence_loss = 0\n",
    "    noobj_confidence_loss = 0\n",
    "    clasification_loss = 0\n",
    "\n",
    "    normalized_pred = normalize_yolo_output(y_pred)#get unsigmoided stuff and tx,ty,tw,th instead of tx,ty,w,h but return this \n",
    "\n",
    "    for b in tf.range(batch_size):\n",
    "        mask = tf.Variable(tf.ones((resolution, resolution, NUM_ANCHORS), dtype=tf.float32))\n",
    "        true_boxes = y_true[b]\n",
    "        confidence_mask = tf.greater(true_boxes[:,4], 0)\n",
    "        true_boxes = tf.boolean_mask(true_boxes, confidence_mask)\n",
    "        dimensions = tf.cast(true_boxes[:, :4], tf.float32)\n",
    "        classifications = tf.cast(true_boxes[:, 5:], tf.float32)\n",
    "\n",
    "        x_cell, y_cell, normalized_true_BB = vectorized_what_grid_cell_it_resides(dimensions, resolution)\n",
    "\n",
    "        for i in tf.range(tf.shape(true_boxes)[0]):\n",
    "            preds = normalized_pred[b, x_cell[i], y_cell[i], :, :4]\n",
    "\n",
    "            ious = iou_vectorized(tf.expand_dims(normalized_true_BB[i], 0), preds)  # (NUM_ANCHORS,)\n",
    "\n",
    "            best_ind = tf.argmax(ious, axis=-1)\n",
    "            best_ind = tf.cast(best_ind, tf.int32)\n",
    "\n",
    "            best_BB = normalized_pred[b, x_cell[i], y_cell[i], best_ind[0], :4]\n",
    "            best_confidence = normalized_pred[b, x_cell[i], y_cell[i], best_ind[0], 4]\n",
    "            best_clasification = normalized_pred[b, x_cell[i], y_cell[i], best_ind[0], 5:]\n",
    "\n",
    "            # Calculate losses\n",
    "            position_loss += tf.constant(1.0, dtype=tf.float32) - ciou(normalized_true_BB[i], best_BB)\n",
    "            obj_confidence_loss += focal_bce(ious[best_ind[0]], tf.reshape(best_confidence, (1,)))\n",
    "            clasification_loss += cce(tf.expand_dims(classifications[i], 0), tf.expand_dims(best_clasification, 0))\n",
    "\n",
    "            pred_boxes = normalized_pred[b, :, :, :, :4]\n",
    "            pred_boxes_flat = tf.reshape(pred_boxes, [-1, 4])\n",
    "\n",
    "            true_box = tf.expand_dims(true_boxes[i,:4],axis=0)\n",
    "\n",
    "            ious = iou_vectorized(true_box, pred_boxes_flat)  # output shape (1, res*res*NUM_ANCHORS)\n",
    "            ious = tf.squeeze(ious, axis=0)  # shape (res*res*NUM_ANCHORS)\n",
    "\n",
    "            iou_mask = ious > 0.7  # boolean mask (res*res*NUM_ANCHORS)\n",
    "\n",
    "            mask_flat = tf.reshape(mask, [-1])  # flatten mask (res*res*NUM_ANCHORS)\n",
    "            mask_flat = tf.where(iou_mask, 0.0, mask_flat)  # zero where iou_mask true\n",
    "            mask.assign(tf.reshape(mask_flat, mask.shape))  # reshape back and assign\n",
    "            mask[x_cell[i], y_cell[i], best_ind[0]].assign(0.0)\n",
    "\n",
    "        #TO DO harsher penalty when the model is predicting ious under a threshold prob 0.4\n",
    "        pred_confidences = normalized_pred[b, :, :, :, 4]  # shape (res, res, NUM_ANCHORS)\n",
    "        zeros = tf.zeros_like(pred_confidences)\n",
    "        bce_losses = focal_bce(zeros, pred_confidences)\n",
    "        weighted_losses = mask * bce_losses\n",
    "\n",
    "        noobj_confidence_loss += tf.reduce_sum(weighted_losses)\n",
    "\n",
    "    total_loss = (position_loss * lambda_coord + \n",
    "              obj_confidence_loss * lambda_obj +\n",
    "              noobj_confidence_loss * lambda_noobj +\n",
    "              clasification_loss * lambda_class)\n",
    "\n",
    "    return total_loss / tf.cast(batch_size, tf.float32)\n",
    "\n",
    "def yolo_multihead_loss(y_trues, y_preds):\n",
    "    \"\"\"\n",
    "    y_trues: (batch, 2 , MAX_BOUNDING_BOXES, 5 + C)\n",
    "    y_preds: list of (nr heads,batch, S, S, B*(5 + C)) tensors\n",
    "    \"\"\"\n",
    "    y_preds_13 = tf.cast(y_preds[0], tf.float32)\n",
    "    y_preds_26 = tf.cast(y_preds[1], tf.float32)\n",
    "    batch_size = tf.shape(y_trues)[0]\n",
    "\n",
    "    y_preds_13 = tf.reshape(y_preds_13, (batch_size, 13, 13, NUM_ANCHORS, 5 + NUM_CLASSES))\n",
    "    y_preds_26 = tf.reshape(y_preds_26, (batch_size, 26, 26, NUM_ANCHORS, 5 + NUM_CLASSES))\n",
    "    \n",
    "    y_13_true = y_trues[:, 0, :, :]\n",
    "    y_26_true = y_trues[:, 1, :, :]\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loss_13 = yolo_loss_single_head(y_13_true, y_preds_13)\n",
    "    loss_26 = yolo_loss_single_head(y_26_true, y_preds_26)\n",
    "\n",
    "    total_loss = loss_13 + loss_26\n",
    "\n",
    "    return tf.cast(total_loss , tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003eb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOMultiHeadModel(tf.keras.Model):\n",
    "    def __init__(self, backbone_model, loss_fn):\n",
    "        super(YOLOMultiHeadModel, self).__init__()\n",
    "        self.backbone = backbone_model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.backbone(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.loss_fn(labels, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        grads_and_vars = []\n",
    "        for grad, var in zip(gradients, self.trainable_variables):\n",
    "            if grad is None:\n",
    "                tf.print(f\"No gradients for variable: {var.name}\")\n",
    "            else:\n",
    "                grads_and_vars.append((grad, var))\n",
    "        self.optimizer.apply_gradients(grads_and_vars)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        tf.print(\"\\n\")\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        predictions = self(images, training=False)\n",
    "        loss = self.loss_fn(labels, predictions)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "model = YOLOMultiHeadModel(backbone, loss_fn=yolo_multihead_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "class MultiHeadMeanIoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, iou_threshold=0.5, name='mean_iou', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.total_iou = self.add_weight(name='total_iou', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        pred_head1, pred_head2 = y_pred\n",
    "        # Combine boxes from both heads\n",
    "        boxes_pred = tf.concat([pred_head1[..., :4], pred_head2[..., :4]], axis=1)  # crude example\n",
    "        boxes_true = tf.concat([y_true[0][..., :4], y_true[1][..., :4]], axis=1)\n",
    "\n",
    "        iou = iou_vectorized(boxes_true, boxes_pred)\n",
    "        mask = tf.cast(iou > self.iou_threshold, tf.float32)\n",
    "\n",
    "        self.total_iou.assign_add(tf.reduce_sum(iou * mask))\n",
    "        self.count.assign_add(tf.reduce_sum(mask))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_iou / (self.count + 1e-6)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_iou.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./checkpoints/epoch_02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def decode_predictions(pred, conf_thresh=0.55):\n",
    "    batch_size, grid, _, anchors, channels = pred.shape\n",
    "    pred = pred[0].numpy()\n",
    "    boxes = []\n",
    "    pixels_per_grid = 416 // grid\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            for k in range(anchors):\n",
    "                obj_score = sigmoid(pred[i, j, k, 4])\n",
    "                if obj_score > conf_thresh:\n",
    "                    bx, by, bw, bh = pred[i, j, k, 0:4]\n",
    "                    class_probs = sigmoid(pred[i, j, k, 5:])\n",
    "                    class_id = np.argmax(class_probs)\n",
    "                    score = obj_score * class_probs[class_id]\n",
    "                    boxes.append([bx + pixels_per_grid * i, by + pixels_per_grid * j, bw, bh, score, class_id])\n",
    "    return boxes\n",
    "\n",
    "def preprocess_image(image_path, img_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (img_size, img_size))\n",
    "    image = image / 255.0  # normalize\n",
    "    return tf.expand_dims(image, axis=0)\n",
    "img_size = 416  # or whatever your model uses\n",
    "image_tensor = preprocess_image(\"./images/official/0AuZ8iMUdcKOVqLFwI4467e4smSjGllWkACcq7jV.jpeg\", img_size)\n",
    "\n",
    "preds = model(image_tensor, training=False)\n",
    "preds = [\n",
    "    normalize_yolo_output(tf.reshape(preds[0],(1, 13, 13, NUM_ANCHORS, 5 + NUM_CLASSES))),\n",
    "    normalize_yolo_output(tf.reshape(preds[1],(1, 26, 26, NUM_ANCHORS, 5 + NUM_CLASSES)))\n",
    "    ]\n",
    "grid_size_13 = preds[0]\n",
    "grid_size_26 = preds[1]\n",
    "\n",
    "filtered_grid_size_13 = decode_predictions(grid_size_13)\n",
    "filtered_grid_size_26 = decode_predictions(grid_size_26)\n",
    "\n",
    "def convert_box(cx, cy, w, h, img_w, img_h):\n",
    "    x = int((cx - w / 2))\n",
    "    y = int((cy - h / 2))\n",
    "    w = int(w)\n",
    "    h = int(h)\n",
    "    return x, y, w, h\n",
    "\n",
    "def plot_boxes(image, boxes, class_names=None):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box in boxes:\n",
    "        cx, cy, w, h, score, class_id = box\n",
    "        x, y, bw, bh = convert_box(cx, cy, w, h, image.shape[1], image.shape[0])\n",
    "        print(x, y, bw, bh)\n",
    "        rect = patches.Rectangle((x, y), bw, bh, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        #label = f\"{class_names[int(class_id)] if class_names else class_id}: {score:.2f}\"\n",
    "        #ax.text(x, y, label, color='white', fontsize=10,\n",
    "        #        bbox=dict(facecolor='black', edgecolor='none', alpha=0.5))\n",
    "    \n",
    "    plt.axis('off')\n",
    "\n",
    "print(image_tensor.shape)\n",
    "plot_boxes(tf.squeeze(image_tensor, axis=0),filtered_grid_size_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e823fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_from_url(url):\n",
    "    \"\"\"Extract a unique ID from the image URL.\"\"\"\n",
    "    # Example: gets '12345' from '.../12345.jpg'\n",
    "    last_index = url.rfind('/')  # Find last occurrence of '/'\n",
    "    last_index2 = url.rfind('.')\n",
    "    result = url[last_index + 1:last_index2]  # Take everything after that\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_id_from_url_with_Extension(url):\n",
    "    \"\"\"Extract a unique ID from the image URL.\"\"\"\n",
    "    # Example: gets '12345' from '.../12345.jpg'\n",
    "    last_index = url.rfind('/')  # Find last occurrence of '/'\n",
    "    result = url[last_index + 1:]  # Take everything after that\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85040456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "save_path_official = \"./images/official\"\n",
    "save_path_unofficial = \"./images/unofficial\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path_official, exist_ok=True)\n",
    "\n",
    "def download_and_save_image(url, save_dir):\n",
    "\n",
    "    image_id = extract_id_from_url(url)\n",
    "    extension = os.path.splitext(url)[1] or \".jpg\"\n",
    "    file_path = os.path.join(save_dir, f\"{image_id}{extension}\")\n",
    "\n",
    "    # Check if file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File already exists, skipping: {file_path}\")\n",
    "        return  # Skip download\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise error for bad status codes\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {url} → {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "urls = [item[\"image_data\"][\"link\"] for item in official_data_formated]\n",
    "\n",
    "MAX_WORKERS = 24\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(download_and_save_image, url, save_path_official) for url in urls]\n",
    "    for future in as_completed(futures):\n",
    "        pass\n",
    "#print(extract_id_from_url(\"https://farm66.staticflickr.com/65535/33978196618_e30a59e0a8_o.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(\"generated_official_data.json\", \"r\") as f:\n",
    "    generated_official_formated = json.load(f)\n",
    "with open(\"rotated_official_data.json\", \"r\") as f:\n",
    "    rotated_official_formated = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b05748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Input data\n",
    "data = official_data_formated[1000]\n",
    "print(data)\n",
    "\n",
    "# Target size\n",
    "TARGET_SIZE = 416\n",
    "\n",
    "# Load image from URL using tensorflow\n",
    "image_path = f'./images/official/{extract_id_from_url_with_Extension(data[\"image_data\"][\"link\"])}'\n",
    "image_pil = Image.open(image_path)\n",
    "image_pil = ImageOps.exif_transpose(image_pil)  # Correct orientation\n",
    "\n",
    "# Convert to NumPy array, then to TensorFlow tensor\n",
    "image_np = np.array(image_pil)\n",
    "image = tf.convert_to_tensor(image_np)\n",
    "\n",
    "# Original size\n",
    "print(data['image_data']['height'])\n",
    "print(data['image_data']['width'])\n",
    "orig_height = data['image_data']['height']\n",
    "orig_width = data['image_data']['width']\n",
    "\n",
    "# Resize image to 416x416\n",
    "image_resized = image\n",
    "image_resized = tf.image.resize(image, [TARGET_SIZE, TARGET_SIZE])\n",
    "image_resized = tf.cast(image_resized, tf.uint8).numpy()\n",
    "\n",
    "# Load bounding box info into pandas\n",
    "bboxes_df = pd.DataFrame(data['bboxes'])\n",
    "\n",
    "scale_x = TARGET_SIZE / orig_width\n",
    "scale_y = TARGET_SIZE / orig_height\n",
    "\n",
    "# Resize bounding boxes accordingly\n",
    "bboxes_df['x'] = bboxes_df['x'] * scale_x\n",
    "bboxes_df['y'] = bboxes_df['y'] * scale_y\n",
    "bboxes_df['w'] = bboxes_df['w'] * scale_x\n",
    "bboxes_df['h'] = bboxes_df['h'] * scale_y\n",
    "\n",
    "# Plot the resized image with bounding box\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image_resized)\n",
    "\n",
    "for idx, row in bboxes_df.iterrows():\n",
    "    x, y, w, h = row['x'], row['y'], row['w'], row['h']\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=False, edgecolor='lime', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y - 10, f\"Cat: {int(row['category'])}\", color='lime', fontsize=10, weight='bold')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(bboxes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "\n",
    "morethanlimit = []\n",
    "limit = 500\n",
    "frequency = 60 * [0]\n",
    "filtered_official_formated = []\n",
    "\n",
    "for buc in official_data_formated + generated_official_formated + rotated_official_formated:\n",
    "    for bbox in buc[\"bboxes\"]:\n",
    "        frequency[bbox[\"category\"]]+=1\n",
    "\n",
    "for i in range(60):\n",
    "    if limit <= frequency[i]:\n",
    "        morethanlimit.append(i)\n",
    "\n",
    "Reassigned_classes = {}\n",
    "for i in range(len(morethanlimit)):\n",
    "    Reassigned_classes[morethanlimit[i]] = i\n",
    "\n",
    "filtered_official_formated_val = []\n",
    "frequency_val = 26 * [0]\n",
    "\n",
    "def adaug_val(bboxes):\n",
    "    tmp = 26*[0]\n",
    "    for bbox in bboxes:\n",
    "        tmp[bbox[\"category\"]]+=1\n",
    "    \n",
    "    for i in range(len(tmp)):\n",
    "        if frequency_val[i] + tmp[i] > 20:\n",
    "            return False\n",
    "        \n",
    "    for i in range(len(tmp)):\n",
    "        frequency_val[i] += tmp[i]\n",
    "    return True\n",
    "\n",
    "\n",
    "for data in generated_official_formated + official_data_formated + rotated_official_formated:\n",
    "    copy = cp.deepcopy(data)\n",
    "    copy[\"bboxes\"] = [\n",
    "        {**bbox, \"category\": Reassigned_classes[bbox[\"category\"]]}\n",
    "        for bbox in copy[\"bboxes\"]\n",
    "        if bbox[\"category\"] in Reassigned_classes\n",
    "    ]\n",
    "    if len(copy[\"bboxes\"]) == 0:\n",
    "        continue\n",
    "    if adaug_val(copy[\"bboxes\"]):\n",
    "        filtered_official_formated_val.append(copy)\n",
    "    else:     \n",
    "        filtered_official_formated.append(copy)\n",
    "\n",
    "print(len(filtered_official_formated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583deabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frequency_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21908917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "TARGET_SIZE = 416\n",
    "def best_anchor_index(box_wh, anchors):\n",
    "    box_area = box_wh[0] * box_wh[1]\n",
    "    anchor_areas = anchors[:, 0] * anchors[:, 1]\n",
    "    inter = np.minimum(anchors[:, 0], box_wh[0]) * np.minimum(anchors[:, 1], box_wh[1])\n",
    "    iou = inter / (box_area + anchor_areas - inter)\n",
    "    return np.argmax(iou)\n",
    "\n",
    "def parser_flat_output(link, height, width, category, x, y, w, h):\n",
    "    # Convert tensors to numpy\n",
    "    link = link.numpy().decode(\"utf-8\")\n",
    "    height = height.numpy()\n",
    "    width = width.numpy()\n",
    "\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "    w = w.numpy()\n",
    "    h = h.numpy()\n",
    "    category = category.numpy()\n",
    "\n",
    "    image_path = './images/official/{}'.format(extract_id_from_url_with_Extension(link))\n",
    "\n",
    "    image_pil = Image.open(image_path)\n",
    "    image_pil = ImageOps.exif_transpose(image_pil)  # Correct orientation\n",
    "    image_pil = image_pil.convert(\"RGB\")\n",
    "\n",
    "    # Convert to NumPy array, then to TensorFlow tensor\n",
    "    image_np = np.array(image_pil)\n",
    "    image = tf.convert_to_tensor(image_np)\n",
    "    image = tf.image.resize(image, size = [TARGET_SIZE, TARGET_SIZE])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    scale_x = TARGET_SIZE / width\n",
    "    scale_y = TARGET_SIZE / height\n",
    "\n",
    "    label_13 = []\n",
    "    label_26 = []\n",
    "    for i in range(len(category)):\n",
    "        formated = (5 + NUM_CLASSES) * [0]\n",
    "        formated[0] = x[i] * scale_x + (w[i] * scale_x) / 2  # center_x\n",
    "        formated[1] = y[i] * scale_y + (h[i] * scale_y) / 2  # center_y\n",
    "        formated[2] = w[i] * scale_x\n",
    "        formated[3] = h[i] * scale_y\n",
    "        formated[4] = 1.0\n",
    "        formated[5 + category[i]] = 1\n",
    "\n",
    "        best_idx = best_anchor_index([formated[2], formated[3]], ANCHORS)\n",
    "        #print(best_idx,extract_id_from_url_with_Extension(link))\n",
    "        for scale, mask in ANCHOR_INDECES.items():\n",
    "            if best_idx in mask:\n",
    "                if scale == 13:\n",
    "                    label_13.append(formated)\n",
    "                    #print(scale,extract_id_from_url_with_Extension(link))\n",
    "                else:\n",
    "                    label_26.append(formated)\n",
    "                    #print(scale,extract_id_from_url_with_Extension(link))\n",
    "                break\n",
    "\n",
    "\n",
    "    max_length = max(len(label_13),len(label_26))\n",
    "    while len(label_13) != max_length:\n",
    "        label_13.append((5 + NUM_CLASSES) * [0])\n",
    "    while len(label_26) != max_length:\n",
    "        label_26.append((5 + NUM_CLASSES) * [0])\n",
    "\n",
    "    label_13 = tf.convert_to_tensor(label_13, dtype=tf.float32)\n",
    "    label_26 = tf.convert_to_tensor(label_26, dtype=tf.float32)\n",
    "\n",
    "    return image, (label_13,label_26)\n",
    "\n",
    "cnt = 0\n",
    "def parser_wrapper(link, height, width, category, x, y, w, h):\n",
    "    image, label = tf.py_function(\n",
    "        func=parser_flat_output,\n",
    "        inp=[link, height, width, category, x, y, w, h],\n",
    "        Tout=(tf.float32, tf.float32)\n",
    "    )\n",
    "    image.set_shape([TARGET_SIZE, TARGET_SIZE, 3])\n",
    "    label.set_shape([2,None, 5 + NUM_CLASSES])\n",
    "    return image, label\n",
    "\n",
    "print(len(filtered_official_formated))\n",
    "print(len(filtered_official_formated_val))\n",
    "\n",
    "def generator_train():\n",
    "    \n",
    "    for official_data in filtered_official_formated:\n",
    "        yield (\n",
    "            official_data['image_data']['link'],\n",
    "            official_data['image_data']['height'],\n",
    "            official_data['image_data']['width'],\n",
    "            [box['category'] for box in official_data['bboxes']],\n",
    "            [box['x'] for box in official_data['bboxes']],\n",
    "            [box['y'] for box in official_data['bboxes']],\n",
    "            [box['w'] for box in official_data['bboxes']],\n",
    "            [box['h'] for box in official_data['bboxes']]\n",
    "        )\n",
    "\n",
    "def generator_val():\n",
    "    \n",
    "    for official_data in filtered_official_formated_val:\n",
    "        yield (\n",
    "            official_data['image_data']['link'],\n",
    "            official_data['image_data']['height'],\n",
    "            official_data['image_data']['width'],\n",
    "            [box['category'] for box in official_data['bboxes']],\n",
    "            [box['x'] for box in official_data['bboxes']],\n",
    "            [box['y'] for box in official_data['bboxes']],\n",
    "            [box['w'] for box in official_data['bboxes']],\n",
    "            [box['h'] for box in official_data['bboxes']]\n",
    "        )\n",
    "\n",
    "# Define output signature correctly\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(), dtype=tf.string),  # link\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32),  # height\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32),  # width\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.int32),  # categories\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32),  # x\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32),  # y\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32),  # w\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32),  # h\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator_train, output_signature=output_signature)\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.map(parser_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.padded_batch(\n",
    "    16,\n",
    "    padded_shapes=(\n",
    "        [TARGET_SIZE, TARGET_SIZE, 3],               # image shape\n",
    "        [2,None, 5 + NUM_CLASSES]      # label shape with variable number of boxes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        0.0,   # image padding (float32)\n",
    "        0.0    # label padding (float32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val = tf.data.Dataset.from_generator(generator_val, output_signature=output_signature)\n",
    "val = val.shuffle(150)\n",
    "val = val.map(parser_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val = val.padded_batch(\n",
    "    16,\n",
    "    padded_shapes=(\n",
    "        [TARGET_SIZE, TARGET_SIZE, 3],               # image shape\n",
    "        [2,None, 5 + NUM_CLASSES]      # label shape with variable number of boxes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        0.0,   # image padding (float32)\n",
    "        0.0    # label padding (float32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ind = 0\n",
    "for image_batch, label_batch in dataset.take(1):\n",
    "    ax.imshow(image_batch[ind])\n",
    "    print(\"Image shape:\", image_batch[ind].shape)\n",
    "    print(\"Image shape:\", label_batch[ind][0])\n",
    "    print(\"Image shape:\", label_batch[ind][1])\n",
    "    print(\"Image shape:\", label_batch[ind].shape)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 416\n",
    "def iou(box, clusters):\n",
    "    # box: [w, h]\n",
    "    # clusters: [[w1, h1], [w2, h2], ...]\n",
    "    x = np.minimum(clusters[:, 0], box[0])\n",
    "    y = np.minimum(clusters[:, 1], box[1])\n",
    "    intersection = x * y\n",
    "    box_area = box[0] * box[1]\n",
    "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "    iou_ = intersection / (box_area + cluster_area - intersection)\n",
    "    return iou_\n",
    "\n",
    "def kmeans(boxes, k, dist=np.median, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    # Initialize clusters randomly\n",
    "    clusters = boxes[np.random.choice(boxes.shape[0], k, replace=False)]\n",
    "    while True:\n",
    "        distances = []\n",
    "        for box in boxes:\n",
    "            distances.append(1 - iou(box, clusters))\n",
    "        distances = np.array(distances)  # shape (num_boxes, k)\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "        new_clusters = []\n",
    "        for cluster_idx in range(k):\n",
    "            cluster_boxes = boxes[nearest_clusters == cluster_idx]\n",
    "            if len(cluster_boxes) == 0:\n",
    "                # Avoid empty clusters by reinitializing randomly\n",
    "                new_clusters.append(clusters[cluster_idx])\n",
    "            else:\n",
    "                new_clusters.append(dist(cluster_boxes, axis=0))\n",
    "        new_clusters = np.array(new_clusters)\n",
    "        if np.all(clusters == new_clusters):\n",
    "            break\n",
    "        clusters = new_clusters\n",
    "    return clusters\n",
    "\n",
    "# Example usage\n",
    "#[[bboxes[2],bboxes[3]] for image_batch, label_batch in dataset for bboxes in label_batch if bboxes[4] == 1]\n",
    "boxes = np.array([\n",
    "    [bbox[\"w\"] * (TARGET_SIZE / dict[\"image_data\"][\"width\"]), bbox[\"h\"] * (TARGET_SIZE / dict[\"image_data\"][\"height\"])]\n",
    "    for dict in filtered_official_formated\n",
    "    for bbox in dict[\"bboxes\"]\n",
    "])\n",
    "\n",
    "print(len(boxes))\n",
    "\n",
    "anchors = kmeans(boxes, k=6)\n",
    "print(\"Anchors:\", anchors)\n",
    "print(np.mean([boxess[0] for boxess in boxes]),np.mean([boxess[1] for boxess in boxes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    for image_batch,label_batch in dataset.take(1):\n",
    "        y_pred = model(image_batch, training=True)\n",
    "        #print(np.array(y_pred).shape)\n",
    "        loss_value = yolo_multihead_loss(label_batch, y_pred)\n",
    "        tf.print(\"Loss:\", loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81661e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"checkpoints/epoch_{epoch:02d}.h5\",  # Save every epoch\n",
    "    save_weights_only=False,  # Set True if you only want to save weights\n",
    "    save_best_only=False,     # Save every epoch, not just the best\n",
    "    verbose=1                 # Print when saving\n",
    ")\n",
    "\n",
    "history = model.fit(dataset,validation_data=val,steps_per_epoch=500, epochs=40, callbacks=[checkpoint],initial_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a99d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a56d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./best_without_metrics.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8233295",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 60 * [0]\n",
    "categories = {}\n",
    "morethanlimit = []\n",
    "limit = 500\n",
    "\n",
    "for dict in official_data_formated + generated_official_formated + rotated_official_formated:\n",
    "    for bbox in dict[\"bboxes\"]:\n",
    "        frequency[bbox[\"category\"]]+=1\n",
    "\n",
    "for i in range(60):\n",
    "    if limit <= frequency[i]:\n",
    "        morethanlimit.append(i)\n",
    "print(frequency)\n",
    "print(morethanlimit)\n",
    "\n",
    "official_data[\"categories\"]\n",
    "for dict in official_data[\"categories\"]:\n",
    "    categories[dict[\"name\"]] = frequency[dict[\"id\"]]\n",
    "\n",
    "\n",
    "# Process data:\n",
    "# - Replace spaces with \\n to make each word appear on a new line\n",
    "# - Sort by frequency (ascending)\n",
    "sorted_items = sorted(categories.items(), key=lambda item: item[1])\n",
    "labels = [label.replace(' ', '\\n') for label, _ in sorted_items]\n",
    "frequencies = [freq for _, freq in sorted_items]\n",
    "\n",
    "# Create spaced x locations\n",
    "x = np.arange(len(labels)) * 2  # Multiply to increase spacing between bars\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(60, 10))\n",
    "plt.bar(x, frequencies, width=1.0, color='skyblue', label='Frequency')\n",
    "\n",
    "# Trend line (metaplot)\n",
    "\n",
    "# Ticks & labels\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('Instance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Instances with Metaplot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f277b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation:\n",
    "#   -rotate 90\n",
    "#   -rotate 180\n",
    "#   -rotate 270\n",
    "#   -zoom in to the object(cu niste background stanga/dreapta/sus/jos fara sa iasa din poza)\n",
    "#   -at least 70% vizible in the right if possible without going out of photo, expand/substract the view randomly onto that point, move the view towards the object if unde 70% without going out\n",
    "#   -at least 70% vizible in the left if possible without going out of photo, expand/substract the view randomly onto that point, move the view towards the object if unde 70% without going out\n",
    "#   -at least 70% vizible in the up if possible without going out of photo, expand/substract the view randomly onto that point, move the view towards the object if unde 70% without going out\n",
    "#   -at least 70% vizible in the down if possible without going out of photo, expand/substract the view randomly onto that point, move the view towards the object if unde 70% without going out\n",
    "#   -mixup doar ca de preferat dupa zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(official_data[\"categories\"])\n",
    "name_categories = {\n",
    "}\n",
    "\n",
    "for category in official_data[\"categories\"]:\n",
    "    name_categories[category[\"id\"]] = category[\"name\"]\n",
    "\n",
    "name_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(morethanlimit))\n",
    "Reassigned_classes = {}\n",
    "for i in range(len(morethanlimit)):\n",
    "    Reassigned_classes[morethanlimit[i]] = i \n",
    "\n",
    "print(Reassigned_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e338e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "BOX_COLOR = (255, 0, 0)  # Red\n",
    "TEXT_COLOR = (255, 255, 255)  # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35,\n",
    "        color=TEXT_COLOR,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "\n",
    "def flip_all_3_directions(dict):\n",
    "    name_image = extract_id_from_url_with_Extension(dict[\"image_data\"][\"link\"])\n",
    "\n",
    "    image = cv2.imread(f\"./images/official/{name_image}\", cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "    bboxes = []\n",
    "    categories = []\n",
    "    for bbox in dict[\"bboxes\"]:\n",
    "        bboxes.append([bbox[\"x\"],bbox[\"y\"],bbox[\"w\"],bbox[\"h\"]])\n",
    "        categories.append(bbox[\"category\"])\n",
    "    \n",
    "    transform1 = A.Compose(\n",
    "    [A.VerticalFlip(p=1)],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "    strict=True,\n",
    "    seed=137,\n",
    "    )\n",
    "    \n",
    "    transform2 = A.Compose(\n",
    "    [A.HorizontalFlip(p=1)],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "    strict=True,\n",
    "    seed=137,\n",
    "    )\n",
    "    transform3 = A.Compose(\n",
    "    [A.HorizontalFlip(p=1),A.VerticalFlip(p=1)],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "    strict=True,\n",
    "    seed=137,\n",
    "    )\n",
    "\n",
    "    transformed1 = transform1(image = image, bboxes = bboxes, category_ids = categories)\n",
    "    transformed2 = transform2(image = image, bboxes = bboxes, category_ids = categories)\n",
    "    transformed3 = transform3(image = image, bboxes = bboxes, category_ids = categories)\n",
    "    print(transformed1)\n",
    "\n",
    "    visualize(\n",
    "    image,\n",
    "    bboxes,\n",
    "    categories,\n",
    "    name_categories\n",
    ")\n",
    "\n",
    "    visualize(\n",
    "    transformed1[\"image\"],\n",
    "    transformed1[\"bboxes\"],\n",
    "    transformed1[\"category_ids\"],\n",
    "    name_categories\n",
    ")\n",
    "    visualize(\n",
    "    transformed2[\"image\"],\n",
    "    transformed2[\"bboxes\"],\n",
    "    transformed2[\"category_ids\"],\n",
    "    name_categories\n",
    ")\n",
    "    visualize(\n",
    "    transformed3[\"image\"],\n",
    "    transformed3[\"bboxes\"],\n",
    "    transformed3[\"category_ids\"],\n",
    "    name_categories\n",
    ")\n",
    "    \n",
    "flip_all_3_directions(official_data_formated[3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(official_data_formated[362][\"bboxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial element show\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "TargetOfEveryCategory = 800\n",
    "numNewPhotos = 0\n",
    "copyfreq = frequency.copy()\n",
    "\n",
    "def overlap_of_box_in_sorrounding(sorrounding, box):\n",
    "        \"\"\"\n",
    "        Measures how much of 'box' is inside 'sorrounding' box (both in COCO format).\n",
    "        Returns a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        x1, y1, w1, h1 = box\n",
    "        x2, y2, w2, h2 = sorrounding\n",
    "\n",
    "        box_x1, box_y1, box_x2, box_y2 = x1, y1, x1 + w1, y1 + h1\n",
    "        surr_x1, surr_y1, surr_x2, surr_y2 = x2, y2, x2 + w2, y2 + h2\n",
    "\n",
    "        inter_x1 = max(box_x1, surr_x1)\n",
    "        inter_y1 = max(box_y1, surr_y1)\n",
    "        inter_x2 = min(box_x2, surr_x2)\n",
    "        inter_y2 = min(box_y2, surr_y2)\n",
    "\n",
    "        inter_w = max(0, inter_x2 - inter_x1)\n",
    "        inter_h = max(0, inter_y2 - inter_y1)\n",
    "        inter_area = inter_w * inter_h\n",
    "\n",
    "        box_area = w1 * h1\n",
    "\n",
    "        return inter_area / box_area if box_area > 0 else 0.0\n",
    "\n",
    "def Shift_in_Multiple_Directions(SorroundingBox,data,chosenPair,LeastBBox = 0.71,MaxExcludedBBox = 0.3):\n",
    "\n",
    "    def CanExpandInthatDirection(ChangedBox):\n",
    "        if data[\"image_data\"][\"width\"] < ChangedBox[0] + ChangedBox[2] or ChangedBox[0] < 0:\n",
    "            return False\n",
    "        if data[\"image_data\"][\"height\"] < ChangedBox[1] + ChangedBox[3] or ChangedBox[1] < 0:\n",
    "            return False\n",
    "        \n",
    "        for i in range(len(chosenPair)):\n",
    "            if chosenPair[i] == 1:\n",
    "                continue\n",
    "            if overlap_of_box_in_sorrounding(ChangedBox,[data[\"bboxes\"][i][\"x\"],data[\"bboxes\"][i][\"y\"],data[\"bboxes\"][i][\"w\"],data[\"bboxes\"][i][\"h\"]]) > MaxExcludedBBox:\n",
    "                return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def GotBelowLeastBBox(ChangedBox):\n",
    "        for i in range(len(chosenPair)):\n",
    "            if chosenPair[i] == 0:\n",
    "                continue\n",
    "            if overlap_of_box_in_sorrounding(ChangedBox,[data[\"bboxes\"][i][\"x\"],data[\"bboxes\"][i][\"y\"],data[\"bboxes\"][i][\"w\"],data[\"bboxes\"][i][\"h\"]]) < LeastBBox:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    combinations = [\n",
    "        [-1, -1],\n",
    "        #[-1,  0],\n",
    "        [-1,  1],\n",
    "        #[ 0, -1],\n",
    "        #[ 0,  1],\n",
    "        [ 1, -1],\n",
    "        #[ 1,  0],\n",
    "        [ 1,  1],\n",
    "    ]#[width,height]\n",
    "\n",
    "    ValidSorroundingBBoxes = []\n",
    "    for combination in combinations:\n",
    "        stillExpanding = True\n",
    "        CopyOfSorroundingBox = SorroundingBox.copy()\n",
    "        while stillExpanding and not GotBelowLeastBBox(CopyOfSorroundingBox):\n",
    "            stillExpanding = False\n",
    "            copy = CopyOfSorroundingBox.copy()\n",
    "            copy[0]+=combination[0]\n",
    "            if CanExpandInthatDirection(copy) and combination[0] != 0:\n",
    "                CopyOfSorroundingBox[0]+=combination[0]\n",
    "                stillExpanding = True\n",
    "            copy = CopyOfSorroundingBox.copy()\n",
    "            copy[1]+=combination[1]\n",
    "            if CanExpandInthatDirection(copy) and combination[1] != 0:\n",
    "                CopyOfSorroundingBox[1]+=combination[1]\n",
    "                stillExpanding = True\n",
    "\n",
    "        if not GotBelowLeastBBox(CopyOfSorroundingBox):\n",
    "            continue\n",
    "\n",
    "        ValidSorroundingBBoxes.append(CopyOfSorroundingBox)\n",
    "\n",
    "    return ValidSorroundingBBoxes\n",
    "\n",
    "\n",
    "\n",
    "def crop_and_normalize(image, surrounding_bbox, bboxes):\n",
    "    \"\"\"\n",
    "    Crops the image using surrounding_bbox and normalizes all given bboxes\n",
    "    relative to the new cropped image.\n",
    "\n",
    "    Args:\n",
    "        image: PIL.Image.Image or NumPy array (H, W, C)\n",
    "        surrounding_bbox: list [x, y, w, h] in COCO format\n",
    "        bboxes: list of bboxes in COCO format [x, y, w, h] — all inside surrounding_bbox\n",
    "\n",
    "    Returns:\n",
    "        cropped_image: PIL.Image.Image (cropped)\n",
    "        normalized_bboxes: list of [x, y, w, h] with coordinates relative to crop\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    sx, sy, sw, sh = surrounding_bbox\n",
    "    cropped_image = image.crop((sx, sy, sx + sw, sy + sh))\n",
    "    width, height = cropped_image.size\n",
    "\n",
    "    normalized_bboxes = []\n",
    "    for dict in bboxes:\n",
    "        copy = dict.copy()\n",
    "        copy[\"x\"] -= sx\n",
    "        copy[\"y\"] -= sy\n",
    "        if copy[\"x\"] < 0:\n",
    "            copy[\"w\"] += copy[\"x\"]  # shrink width (x is negative)\n",
    "            copy[\"x\"] = 0   # clamp x\n",
    "        if copy[\"y\"] < 0:\n",
    "            copy[\"h\"] += copy[\"y\"]  # shrink height (y is negative)\n",
    "            copy[\"y\"] = 0   # clamp y\n",
    "        copy[\"w\"] = min(copy[\"w\"], width - copy[\"x\"])\n",
    "        copy[\"h\"] = min(copy[\"h\"], height - copy[\"y\"])\n",
    "        normalized_bboxes.append(copy)\n",
    "        \n",
    "    normalizedInstance = {\n",
    "        \"image_data\":{\n",
    "            \"link\":image,\n",
    "            \"width\":width,\n",
    "            \"height\":height,\n",
    "        },\n",
    "        \"bboxes\":normalized_bboxes\n",
    "    }\n",
    "\n",
    "    return cropped_image, normalizedInstance\n",
    "\n",
    "def valid_pair(data,chosenPair):\n",
    "\n",
    "    def expand_bbox(bbox, img_width, img_height, initial_pad=100, min_size=416):\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Initial corners with padding\n",
    "        x1 = max(0, x - initial_pad)\n",
    "        y1 = max(0, y - initial_pad)\n",
    "        x2 = min(img_width, x + w + initial_pad)\n",
    "        y2 = min(img_height, y + h + initial_pad)\n",
    "\n",
    "        # Expand width until at least 416\n",
    "        while (x2 - x1) < min_size:\n",
    "            if x1 > 0:\n",
    "                x1 -= 1\n",
    "            if (x2 - x1) < min_size and x2 < img_width:\n",
    "                x2 += 1\n",
    "            # Stop if stuck\n",
    "            if x1 == 0 and x2 == img_width:\n",
    "                break\n",
    "\n",
    "        # Expand height until at least 416\n",
    "        while (y2 - y1) < min_size:\n",
    "            if y1 > 0:\n",
    "                y1 -= 1\n",
    "            if (y2 - y1) < min_size and y2 < img_height:\n",
    "                y2 += 1\n",
    "            # Stop if stuck\n",
    "            if y1 == 0 and y2 == img_height:\n",
    "                break\n",
    "\n",
    "        # Final COCO format [x, y, w, h]\n",
    "        return [x1, y1, x2 - x1, y2 - y1]\n",
    "\n",
    "    def getSorroundingBBox(boxes):\n",
    "        # Convert to x_min, y_min, x_max, y_max\n",
    "        x_mins = [box[0] for box in boxes]\n",
    "        y_mins = [box[1] for box in boxes]\n",
    "        x_maxs = [box[0] + box[2] for box in boxes]\n",
    "        y_maxs = [box[1] + box[3] for box in boxes]\n",
    "\n",
    "        # Get enclosing box\n",
    "        x_min_enclosing = min(x_mins)\n",
    "        y_min_enclosing = min(y_mins)\n",
    "        x_max_enclosing = max(x_maxs)\n",
    "        y_max_enclosing = max(y_maxs)\n",
    "\n",
    "        # Convert back to COCO format: [x, y, width, height]\n",
    "        enclosing_box = [\n",
    "            x_min_enclosing,\n",
    "            y_min_enclosing,\n",
    "            x_max_enclosing - x_min_enclosing,\n",
    "            y_max_enclosing - y_min_enclosing\n",
    "        ]\n",
    "\n",
    "        return enclosing_box\n",
    "    \n",
    "    all_selected__bboxes = []\n",
    "\n",
    "    for i in range(len(chosenPair)):\n",
    "        if chosenPair[i] == 0:\n",
    "            continue\n",
    "        if copyfreq[data[\"bboxes\"][i][\"category\"]] > TargetOfEveryCategory:\n",
    "            return False\n",
    "        all_selected__bboxes.append([data[\"bboxes\"][i][\"x\"],data[\"bboxes\"][i][\"y\"],data[\"bboxes\"][i][\"w\"],data[\"bboxes\"][i][\"h\"]])\n",
    "    \n",
    "    SorroundingBox = getSorroundingBBox(all_selected__bboxes)\n",
    "    SorroundingBox = expand_bbox(SorroundingBox,data[\"image_data\"][\"width\"],data[\"image_data\"][\"height\"])\n",
    "    \n",
    "    for i in range(len(chosenPair)):\n",
    "        if chosenPair[i] == 1:\n",
    "            continue\n",
    "        if overlap_of_box_in_sorrounding(SorroundingBox,[data[\"bboxes\"][i][\"x\"],data[\"bboxes\"][i][\"y\"],data[\"bboxes\"][i][\"w\"],data[\"bboxes\"][i][\"h\"]]) > 0.3:\n",
    "            return False\n",
    "\n",
    "    #can happen to not be minimum requierment of 416x416\n",
    "    return Shift_in_Multiple_Directions(SorroundingBox,data,chosenPair)\n",
    "\n",
    "tries = 0\n",
    "\n",
    "generated_official_formated = []\n",
    "\n",
    "def generate_element_pairs(data):\n",
    "\n",
    "    image = Image.open(f\"images/official/{extract_id_from_url_with_Extension(data[\"image_data\"][\"link\"])}\")\n",
    "    image = ImageOps.exif_transpose(image)  # Correct orientation\n",
    "\n",
    "    sol = np.array(len(data[\"bboxes\"]) * [0])\n",
    "    sol[len(sol)-1] = 1\n",
    "    global tries\n",
    "    tries = 0\n",
    "    while not np.all(sol == 1) and tries < 1e6:\n",
    "        tries +=1\n",
    "\n",
    "        for i in range(len(sol)):\n",
    "            if copyfreq[data[\"bboxes\"][i][\"category\"]] >= TargetOfEveryCategory:\n",
    "                sol[i] = 1\n",
    "\n",
    "        flipped = 1 - sol\n",
    "\n",
    "        if np.all(flipped == 0):\n",
    "            break\n",
    "\n",
    "        result = valid_pair(data, flipped)\n",
    "        #print(flipped)\n",
    "        if not isinstance(result, bool):\n",
    "            tries = 0\n",
    "            global copyfreq\n",
    "            chosenBBoxes = []\n",
    "            for i in range(len(flipped)):\n",
    "                if flipped[i] == 1:\n",
    "                    chosenBBoxes.append(data[\"bboxes\"][i])\n",
    "                    copyfreq[data[\"bboxes\"][i][\"category\"]] += len(result)\n",
    "\n",
    "            for Sourround in result:\n",
    "                global numNewPhotos\n",
    "                numNewPhotos+=1\n",
    "                cropped_image,normalizedInstance = crop_and_normalize(image,Sourround,chosenBBoxes)\n",
    "                cropped_image = cropped_image.convert(\"RGB\")\n",
    "\n",
    "                normalizedInstance[\"image_data\"][\"link\"] = f\"generated/zommed_{numNewPhotos}.png\"\n",
    "                cropped_image.save(f\"./images/official/zommed_{numNewPhotos}.png\")\n",
    "                generated_official_formated.append(normalizedInstance)\n",
    "\n",
    "            #    #print(normalizedInstance)\n",
    "            #    bboxes_df = pd.DataFrame(normalizedInstance['bboxes'])\n",
    "            #    # Plot the resized image with bounding box\n",
    "            #    fig, ax = plt.subplots(1)\n",
    "            #    ax.imshow(cropped_image)\n",
    "            #    for idx, row in bboxes_df.iterrows():\n",
    "            #        x, y, w, h = row['x'], row['y'], row['w'], row['h']\n",
    "            #        rect = plt.Rectangle((x, y), w, h, fill=False, edgecolor='lime', linewidth=2)\n",
    "            #        ax.add_patch(rect)\n",
    "            #        ax.text(x, y - 10, f\"Cat: {int(row['category'])}\", color='lime', fontsize=10, weight='bold')\n",
    "            #    plt.axis('off')\n",
    "            #    plt.show()\n",
    "\n",
    "        #print(\"----------------------------------\")\n",
    "\n",
    "        ind = len(sol)-1\n",
    "        while sol[ind] == 1:\n",
    "            sol[ind] = 0\n",
    "            ind-=1\n",
    "        sol[ind] = 1\n",
    "\n",
    "#print(len(official_data_formated[326][\"bboxes\"]))\n",
    "current = 0\n",
    "for data in official_data_formated:\n",
    "    current+=1\n",
    "    print(current)\n",
    "    generate_element_pairs(data)\n",
    "#print(numNewPhotos)\n",
    "#print(copyfreq)\n",
    "\n",
    "import json\n",
    "with open(\"generated_official_data.json\", \"w\") as f:\n",
    "    json.dump(generated_official_formated, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "numNewPhotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ffcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(copyfreq)\n",
    "a_second_freqcopy = copyfreq.copy()\n",
    "print(a_second_freqcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "numNewRotatedPhotos = 0\n",
    "a_second_freqcopy = copyfreq.copy()\n",
    "MaximumTargetToAllCategories = 1000\n",
    "\n",
    "transform1 = A.Compose(\n",
    "[A.VerticalFlip(p=1)],\n",
    "bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "strict=True,\n",
    "seed=137,\n",
    ")\n",
    "\n",
    "transform2 = A.Compose(\n",
    "[A.HorizontalFlip(p=1)],\n",
    "bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "strict=True,\n",
    "seed=137,\n",
    ")\n",
    "transform3 = A.Compose(\n",
    "[A.HorizontalFlip(p=1),A.VerticalFlip(p=1)],\n",
    "bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    "strict=True,\n",
    "seed=137,\n",
    ")\n",
    "\n",
    "rotated_official_formated = []\n",
    "\n",
    "def verify_frequency(data):\n",
    "    counts = {}\n",
    "    for bbox in data[\"bboxes\"]:\n",
    "        cat = bbox[\"category\"]\n",
    "        counts[cat] = counts.get(cat, 0) + 1  # count how many times each category appears\n",
    "\n",
    "    for cat, count in counts.items():\n",
    "        if a_second_freqcopy[cat] + (count * 3) > MaximumTargetToAllCategories:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def make_formated(image,bboxes,catgories):\n",
    "    global numNewRotatedPhotos\n",
    "    global a_second_freqcopy\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    categ = []\n",
    "    for cat in catgories:\n",
    "        a_second_freqcopy[int(cat)] += 1\n",
    "        categ.append(a_second_freqcopy[int(cat)])\n",
    "    print(categ)\n",
    "    print(catgories)\n",
    "    print(len(bboxes))\n",
    "\n",
    "    width, height = image.size\n",
    "    image.save(f\"./images/official/rotated_{numNewRotatedPhotos}.png\")\n",
    "\n",
    "    return {\n",
    "        \"image_data\":{\n",
    "            \"link\":f\"generated/rotated_{numNewRotatedPhotos}.png\",\n",
    "            \"width\":width,\n",
    "            \"height\":height,\n",
    "        },\n",
    "        \"bboxes\":[{\"x\":bboxes[i][0],\"y\":bboxes[i][1],\"w\":bboxes[i][2],\"h\":bboxes[i][3],\"category\":int(catgories[i])} for i in range(len(bboxes))]\n",
    "    }\n",
    "\n",
    "def rotate_in_all_directions(data):#formated\n",
    "    image = Image.open(f\"images/official/{extract_id_from_url_with_Extension(data[\"image_data\"][\"link\"])}\")\n",
    "    image = ImageOps.exif_transpose(image)  # Correct orientation\n",
    "    image = image.convert(\"RGB\")\n",
    "    np_cropped_image = np.array(image).astype(np.uint8)\n",
    "    bboxes = []\n",
    "    categories = []\n",
    "    for bbox in data[\"bboxes\"]:\n",
    "        bboxes.append([bbox[\"x\"],bbox[\"y\"],bbox[\"w\"],bbox[\"h\"]])\n",
    "        categories.append(bbox[\"category\"])\n",
    "\n",
    "    global numNewRotatedPhotos\n",
    "    numNewRotatedPhotos +=1\n",
    "    transformed1 = transform1(image = np_cropped_image, bboxes = bboxes, category_ids = categories)\n",
    "    rotated_official_formated.append(make_formated(transformed1[\"image\"],transformed1[\"bboxes\"],transformed1[\"category_ids\"]))\n",
    "    numNewRotatedPhotos +=1\n",
    "    transformed2 = transform2(image = np_cropped_image, bboxes = bboxes, category_ids = categories)\n",
    "    rotated_official_formated.append(make_formated(transformed2[\"image\"],transformed2[\"bboxes\"],transformed2[\"category_ids\"]))\n",
    "    numNewRotatedPhotos +=1\n",
    "    transformed3 = transform3(image = np_cropped_image, bboxes = bboxes, category_ids = categories)\n",
    "    rotated_official_formated.append(make_formated(transformed3[\"image\"],transformed3[\"bboxes\"],transformed3[\"category_ids\"]))\n",
    "\n",
    "current = 0\n",
    "print(a_second_freqcopy)\n",
    "for data in official_data_formated + generated_official_formated:\n",
    "    current+=1\n",
    "    if verify_frequency(data):\n",
    "        rotate_in_all_directions(data)\n",
    "        print(current,\"accept\")\n",
    "    else:\n",
    "        print(current,\"skipped\")\n",
    "\n",
    "import json\n",
    "with open(\"rotated_official_data.json\", \"w\") as f:\n",
    "    json.dump(rotated_official_formated, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "numNewRotatedPhotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c24294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "print(numNewPhotos)\n",
    "\n",
    "official_data[\"categories\"]\n",
    "for dict in official_data[\"categories\"]:\n",
    "    categories[dict[\"name\"]] = a_second_freqcopy[dict[\"id\"]]\n",
    "\n",
    "\n",
    "# Process data:\n",
    "# - Replace spaces with \\n to make each word appear on a new line\n",
    "# - Sort by frequency (ascending)\n",
    "sorted_items = sorted(categories.items(), key=lambda item: item[1])\n",
    "labels = [label.replace(' ', '\\n') for label, _ in sorted_items]\n",
    "frequencies = [freq for _, freq in sorted_items]\n",
    "\n",
    "# Create spaced x locations\n",
    "x = np.arange(len(labels)) * 2  # Multiply to increase spacing between bars\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(60, 10))\n",
    "plt.bar(x, frequencies, width=1.0, color='skyblue', label='Frequency')\n",
    "\n",
    "# Trend line (metaplot)\n",
    "\n",
    "# Ticks & labels\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('Instance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Instances with Metaplot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_official_formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34786c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
